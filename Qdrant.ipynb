{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNoW4xvwEFGFn/U+IEWOz3X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daniyal1d/Qdrant/blob/main/Qdrant.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KS4Z6wseiqRU",
        "outputId": "b0a054b8-3edc-4e0f-dc24-63012de0a0eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/327.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m327.7/327.7 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q google-generativeai qdrant-client numpy requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "from qdrant_client import QdrantClient\n",
        "import google.generativeai as genai\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.http import models as rest\n",
        "import numpy as np\n",
        "import requests\n",
        "# STEP 3: Gemini API Key setup"
      ],
      "metadata": {
        "id": "OJs7ba9niwRK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "genai.configure(api_key=\"\")\n",
        "model = genai.GenerativeModel(\"gemini-2.0-flash\")"
      ],
      "metadata": {
        "id": "Q5_l_Wl4i1WZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qdrant_client = QdrantClient(\n",
        "    url=\"\",\n",
        "    api_key=\"\"  # Replace with your real API key\n",
        ")\n",
        "\n",
        "collection_name = \"ChatMemory\"\n",
        "\n",
        "# Create collection if it doesn't exist\n",
        "if collection_name not in [c.name for c in qdrant_client.get_collections().collections]:\n",
        "    qdrant_client.recreate_collection(\n",
        "        collection_name=collection_name,\n",
        "        vectors_config=rest.VectorParams(size=768, distance=rest.Distance.COSINE),\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9_Iq0bNi2H5",
        "outputId": "5ce79918-7c1f-483e-ee11-cdb4ad6ba6bf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-410c829ed3eb>:10: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
            "  qdrant_client.recreate_collection(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def embed(text):\n",
        "    res = genai.embed_content(\n",
        "        model=\"models/embedding-001\",\n",
        "        content=text,\n",
        "        task_type=\"RETRIEVAL_DOCUMENT\"\n",
        "    )\n",
        "    return np.array(res[\"embedding\"], dtype=np.float32)"
      ],
      "metadata": {
        "id": "5Y0_gzepi5Cg"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_to_memory(text):\n",
        "    vector = embed(text)\n",
        "    # Use text hash or UUID for unique ID in production; here text is used as id for demo\n",
        "    qdrant_client.upsert(\n",
        "        collection_name=collection_name,\n",
        "        points=[\n",
        "            rest.PointStruct(id=hash(text) % (10**9), vector=vector.tolist(), payload={\"text\": text})\n",
        "        ],\n",
        "    )"
      ],
      "metadata": {
        "id": "SXdSHUj6i7Nc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search_memory(query, top_k=3):\n",
        "    vector = embed(query).tolist()\n",
        "    search_result = qdrant_client.search(\n",
        "        collection_name=collection_name,\n",
        "        query_vector=vector,\n",
        "        limit=top_k,\n",
        "        with_payload=True,\n",
        "    )\n",
        "    return [point.payload[\"text\"] for point in search_result]"
      ],
      "metadata": {
        "id": "vJJMXO5Pi-v_"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat(user_input, history=[]):\n",
        "    context = search_memory(user_input)\n",
        "    context_str = \"\\n\".join(context)\n",
        "\n",
        "    prompt = f\"\"\"You are a helpful assistant. Use the following context to answer the user's question.\n",
        "\n",
        "Context:\n",
        "{context_str}\n",
        "\n",
        "User: {user_input}\n",
        "\"\"\"\n",
        "\n",
        "    response = model.generate_content(prompt)\n",
        "    reply = response.text\n",
        "\n",
        "    # Store user input and model reply\n",
        "    add_to_memory(user_input)\n",
        "    add_to_memory(reply)\n",
        "\n",
        "    history.append((user_input, reply))\n",
        "    return reply"
      ],
      "metadata": {
        "id": "S1IfosYBi_pJ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ü§ñ Gemini Chatbot with Qdrant memory (type 'exit' to quit)\\n\")\n",
        "chat_history = []\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() == \"exit\":\n",
        "        print(\"Goodbye!\")\n",
        "        break\n",
        "    response = chat(user_input, chat_history)\n",
        "    print(f\"Assistant: {response}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        },
        "id": "3ZXAMIT7jBYs",
        "outputId": "64810e3a-3672-4df0-bf83-84c3b765f01f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü§ñ Gemini Chatbot with Qdrant memory (type 'exit' to quit)\n",
            "\n",
            "You: hello \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-3a3d4bbbfd08>:3: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
            "  search_result = qdrant_client.search(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assistant: Hello! How can I help you today?\n",
            "\n",
            "\n",
            "You: what is mcp\n",
            "Assistant: I am sorry, I cannot answer that question. The context does not provide information about what \"mcp\" means.\n",
            "\n",
            "\n",
            "You: model context protocol\n",
            "Assistant: I am not familiar with an MCP with that definition. Can you provide more context? \n",
            "\n",
            "You: what can you do \n",
            "Assistant: I can provide information, answer questions, generate text, translate languages, and complete tasks as instructed. I am still under development, but I am learning new things every day.\n",
            "\n",
            "\n",
            "You: what information you ccan provide me\n",
            "Assistant: Hello! How can I help you today?\n",
            "\n",
            "\n",
            "You: exit\n",
            "Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R07qICilk-0G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}