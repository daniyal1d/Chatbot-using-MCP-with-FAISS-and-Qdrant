{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO2LomNKxoZmG3HRXmKOHEv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daniyal1d/Chatbot-using-MCP-with-FAISS-and-Qdrant/blob/main/Chatbot_using_MCP_with_FAISS_and_Qdrant.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILY0GSU9vEw5",
        "outputId": "6fe43854-6886-4529-f4b0-f172b57ee84c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl (31.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q google-generativeai faiss-cpu\n"
      ],
      "metadata": {
        "id": "hr3bvVqnvGF5"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "import faiss\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import os\n"
      ],
      "metadata": {
        "id": "V7jevR3dvIUO"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "genai.configure(api_key=\"AIzaSyDzZ4HcRozg43gnrKZdPlArjnjCC1NjcKk\")\n",
        "model = genai.GenerativeModel(\"gemini-2.0-flash\")"
      ],
      "metadata": {
        "id": "2AtGAm21vIkM"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In-memory vector store\n",
        "texts = []  # To keep track of texts (chat memory)\n",
        "dimension = 768  # Gemini embedding-001 returns 768-dim vectors\n",
        "index = faiss.IndexFlatL2(dimension)  # L2 similarity index"
      ],
      "metadata": {
        "id": "lJbhzcmdvI23"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding function using Gemini\n",
        "def embed(text):\n",
        "    res = genai.embed_content(\n",
        "        model=\"models/embedding-001\",\n",
        "        content=text,\n",
        "        task_type=\"RETRIEVAL_DOCUMENT\"\n",
        "    )\n",
        "    return np.array(res[\"embedding\"], dtype=np.float32)"
      ],
      "metadata": {
        "id": "0R8Lk_gAvJUv"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add text to memory\n",
        "def add_to_memory(text):\n",
        "    vector = embed(text)\n",
        "    index.add(np.array([vector]))\n",
        "    texts.append(text)"
      ],
      "metadata": {
        "id": "rMPjFxDPvkuV"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Search memory\n",
        "def search_memory(query, top_k=3):\n",
        "    if len(texts) == 0:\n",
        "        return []\n",
        "\n",
        "    query_vector = embed(query).reshape(1, -1)\n",
        "    distances, indices = index.search(query_vector, top_k)\n",
        "\n",
        "    return [texts[i] for i in indices[0] if i < len(texts)]"
      ],
      "metadata": {
        "id": "5mouPJHlvlVf"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chat function\n",
        "def chat(user_input, history=[]):\n",
        "    context = search_memory(user_input)\n",
        "    context_str = \"\\n\".join(context)\n",
        "\n",
        "    prompt = f\"\"\"You are a helpful assistant. Use the following context to answer the user's question.\n",
        "\n",
        "Context:\n",
        "{context_str}\n",
        "\n",
        "User: {user_input}\n",
        "\"\"\"\n",
        "\n",
        "    response = model.generate_content(prompt)\n",
        "    reply = response.text\n",
        "\n",
        "    add_to_memory(user_input)\n",
        "    add_to_memory(reply)\n",
        "\n",
        "    history.append((user_input, reply))\n",
        "    return reply"
      ],
      "metadata": {
        "id": "02MAD7zSvlQ2"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Interactive chat\n",
        "print(\"ü§ñ Gemini Chatbot with FAISS (type 'exit' to quit)\\n\")\n",
        "chat_history = []\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() == \"exit\":\n",
        "        print(\"Goodbye!\")\n",
        "        break\n",
        "    response = chat(user_input, chat_history)\n",
        "    print(f\"Assistant: {response}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iE_LNWPAvlKg",
        "outputId": "513d3ab6-c858-46b6-c3ba-d42630e2fbc1"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü§ñ Gemini Chatbot with FAISS (type 'exit' to quit)\n",
            "\n",
            "You: hello\n",
            "Assistant: Hello! How can I help you today?\n",
            "\n",
            "\n",
            "You: what can you do for me\n",
            "Assistant: Hello! I can answer questions, provide information, generate text, summarize content, translate languages, and much more. Just let me know what you need!\n",
            "\n",
            "\n",
            "You: provide information\n",
            "Assistant: Okay! I can provide information. What information are you looking for? To give you the best answer, please be as specific as possible about the topic you're interested in.\n",
            "\n",
            "\n",
            "You: i need information about Pakistan\n",
            "Assistant: Okay! To give you the best information about Pakistan, could you be more specific about what you'd like to know? For example, are you interested in:\n",
            "\n",
            "*   **General information:** (e.g., geography, climate, population, culture, history, government)\n",
            "*   **Economy:** (e.g., major industries, trade, economic growth)\n",
            "*   **Travel:** (e.g., tourist attractions, visa requirements, safety)\n",
            "*   **Culture:** (e.g., languages, religions, cuisine, arts)\n",
            "*   **Current events:** (e.g., recent news, political developments)\n",
            "*   **Specific cities or regions:** (e.g., Lahore, Karachi, Islamabad, Punjab, Sindh)\n",
            "*   **History:** (e.g., independence, wars, significant figures)\n",
            "\n",
            "The more specific you are, the better I can tailor my response to your needs!\n",
            "\n",
            "\n",
            "You: i need just general information\n",
            "Assistant: Okay, I can provide general information. To help me narrow it down and provide something useful, could you tell me what general topic area you're interested in? For example, are you interested in:\n",
            "\n",
            "*   **General knowledge/trivia?**\n",
            "*   **Current events?**\n",
            "*   **A specific subject like history, science, or art?**\n",
            "*   **Something else entirely?**\n",
            "\n",
            "The more information you give me, the better I can assist you.\n",
            "\n",
            "\n",
            "You: just provifr general information about pakistan\n",
            "Assistant: Okay, here's some general information about Pakistan:\n",
            "\n",
            "*   **Official Name:** Islamic Republic of Pakistan\n",
            "*   **Location:** South Asia\n",
            "*   **Capital:** Islamabad\n",
            "*   **Largest City:** Karachi\n",
            "*   **Official Languages:** Urdu, English\n",
            "*   **Currency:** Pakistani Rupee (PKR)\n",
            "*   **Government:** Parliamentary Republic\n",
            "*   **Population:** Estimated to be over 240 million.\n",
            "*   **Major Religions:** Islam (majority), with minorities of Hinduism, Christianity, and others.\n",
            "*   **Brief History:** Pakistan was created in 1947 as a separate nation for Muslims of British India. It has a rich history with roots in the Indus Valley Civilization.\n",
            "*   **Geography:** Diverse, including plains, deserts, forests, hills, and mountains (including the Himalayas and Karakoram range).\n",
            "*   **Economy:** Developing country with a mixed economy. Key sectors include agriculture, textiles, and services.\n",
            "\n",
            "You: exit\n",
            "Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O9shwpw7vk_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KS4Z6wseiqRU",
        "outputId": "b0a054b8-3edc-4e0f-dc24-63012de0a0eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/327.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m327.7/327.7 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q google-generativeai qdrant-client numpy requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "from qdrant_client import QdrantClient\n",
        "import google.generativeai as genai\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.http import models as rest\n",
        "import numpy as np\n",
        "import requests\n",
        "# STEP 3: Gemini API Key setup"
      ],
      "metadata": {
        "id": "OJs7ba9niwRK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "genai.configure(api_key=\"\")\n",
        "model = genai.GenerativeModel(\"gemini-2.0-flash\")"
      ],
      "metadata": {
        "id": "Q5_l_Wl4i1WZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qdrant_client = QdrantClient(\n",
        "    url=\"\",\n",
        "    api_key=\"\"  # Replace with your real API key\n",
        ")\n",
        "\n",
        "collection_name = \"ChatMemory\"\n",
        "\n",
        "# Create collection if it doesn't exist\n",
        "if collection_name not in [c.name for c in qdrant_client.get_collections().collections]:\n",
        "    qdrant_client.recreate_collection(\n",
        "        collection_name=collection_name,\n",
        "        vectors_config=rest.VectorParams(size=768, distance=rest.Distance.COSINE),\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9_Iq0bNi2H5",
        "outputId": "5ce79918-7c1f-483e-ee11-cdb4ad6ba6bf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-410c829ed3eb>:10: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
            "  qdrant_client.recreate_collection(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def embed(text):\n",
        "    res = genai.embed_content(\n",
        "        model=\"models/embedding-001\",\n",
        "        content=text,\n",
        "        task_type=\"RETRIEVAL_DOCUMENT\"\n",
        "    )\n",
        "    return np.array(res[\"embedding\"], dtype=np.float32)"
      ],
      "metadata": {
        "id": "5Y0_gzepi5Cg"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_to_memory(text):\n",
        "    vector = embed(text)\n",
        "    # Use text hash or UUID for unique ID in production; here text is used as id for demo\n",
        "    qdrant_client.upsert(\n",
        "        collection_name=collection_name,\n",
        "        points=[\n",
        "            rest.PointStruct(id=hash(text) % (10**9), vector=vector.tolist(), payload={\"text\": text})\n",
        "        ],\n",
        "    )"
      ],
      "metadata": {
        "id": "SXdSHUj6i7Nc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search_memory(query, top_k=3):\n",
        "    vector = embed(query).tolist()\n",
        "    search_result = qdrant_client.search(\n",
        "        collection_name=collection_name,\n",
        "        query_vector=vector,\n",
        "        limit=top_k,\n",
        "        with_payload=True,\n",
        "    )\n",
        "    return [point.payload[\"text\"] for point in search_result]"
      ],
      "metadata": {
        "id": "vJJMXO5Pi-v_"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat(user_input, history=[]):\n",
        "    context = search_memory(user_input)\n",
        "    context_str = \"\\n\".join(context)\n",
        "\n",
        "    prompt = f\"\"\"You are a helpful assistant. Use the following context to answer the user's question.\n",
        "\n",
        "Context:\n",
        "{context_str}\n",
        "\n",
        "User: {user_input}\n",
        "\"\"\"\n",
        "\n",
        "    response = model.generate_content(prompt)\n",
        "    reply = response.text\n",
        "\n",
        "    # Store user input and model reply\n",
        "    add_to_memory(user_input)\n",
        "    add_to_memory(reply)\n",
        "\n",
        "    history.append((user_input, reply))\n",
        "    return reply"
      ],
      "metadata": {
        "id": "S1IfosYBi_pJ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ü§ñ Gemini Chatbot with Qdrant memory (type 'exit' to quit)\\n\")\n",
        "chat_history = []\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() == \"exit\":\n",
        "        print(\"Goodbye!\")\n",
        "        break\n",
        "    response = chat(user_input, chat_history)\n",
        "    print(f\"Assistant: {response}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        },
        "id": "3ZXAMIT7jBYs",
        "outputId": "64810e3a-3672-4df0-bf83-84c3b765f01f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü§ñ Gemini Chatbot with Qdrant memory (type 'exit' to quit)\n",
            "\n",
            "You: hello \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-3a3d4bbbfd08>:3: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
            "  search_result = qdrant_client.search(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assistant: Hello! How can I help you today?\n",
            "\n",
            "\n",
            "You: what is mcp\n",
            "Assistant: I am sorry, I cannot answer that question. The context does not provide information about what \"mcp\" means.\n",
            "\n",
            "\n",
            "You: model context protocol\n",
            "Assistant: I am not familiar with an MCP with that definition. Can you provide more context? \n",
            "\n",
            "You: what can you do \n",
            "Assistant: I can provide information, answer questions, generate text, translate languages, and complete tasks as instructed. I am still under development, but I am learning new things every day.\n",
            "\n",
            "\n",
            "You: what information you ccan provide me\n",
            "Assistant: Hello! How can I help you today?\n",
            "\n",
            "\n",
            "You: exit\n",
            "Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R07qICilk-0G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}